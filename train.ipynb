{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from dataset import load_core_set_data, load_pca_data\n",
    "from simulation.simulator import FDTDSimulator\n",
    "from simulation.student import LSTMPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FDTDDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.FloatTensor(data)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_core_set_data()\n",
    "\n",
    "train_dataset = FDTDDataset(train_data, train_labels)\n",
    "test_dataset = FDTDDataset(test_data, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.8255, 7.3699, 2.8815, 1.5214, 6.0255, 8.2805, 4.9387, 7.2811, 2.4591,\n",
      "         3.9089],\n",
      "        [2.0647, 2.0806, 6.3558, 2.2175, 8.8540, 2.2342, 6.0093, 8.0540, 1.7990,\n",
      "         0.4360],\n",
      "        [8.0274, 4.3012, 2.9524, 9.2272, 8.8699, 8.8332, 9.2471, 5.4805, 2.6575,\n",
      "         6.1031],\n",
      "        [9.7515, 8.3529, 1.8866, 3.3360, 2.4874, 2.4479, 2.3108, 1.9156, 4.2194,\n",
      "         7.2804],\n",
      "        [2.8730, 0.7302, 2.6532, 6.2523, 7.1793, 7.8691, 4.6819, 4.6316, 2.2732,\n",
      "         8.7599],\n",
      "        [5.4082, 3.4759, 3.0826, 2.8230, 6.7669, 2.7832, 7.1524, 9.9375, 8.7169,\n",
      "         4.4255],\n",
      "        [3.3696, 9.6699, 0.1420, 0.8477, 7.8621, 7.3240, 7.9770, 4.6472, 8.7734,\n",
      "         0.1351],\n",
      "        [4.6822, 4.4650, 5.7432, 3.8443, 6.9914, 6.0555, 1.5298, 9.7675, 5.2538,\n",
      "         9.0771],\n",
      "        [6.5631, 5.8581, 9.5429, 6.5055, 7.0407, 0.5696, 5.2386, 3.3710, 6.9758,\n",
      "         9.2995],\n",
      "        [9.5105, 2.2587, 4.7564, 8.3978, 5.8785, 7.3033, 5.8868, 6.6483, 4.4559,\n",
      "         1.6768]])\n"
     ]
    }
   ],
   "source": [
    "radius_matrix = torch.rand(10, 10) * 10\n",
    "\n",
    "print(radius_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LSTMPredictor(\n",
    "    input_size=10,\n",
    "    hidden_size=128,\n",
    "    output_size=10,\n",
    "    num_layers=2,\n",
    "    dropout=0.1,\n",
    "    device=device\n",
    ")\n",
    "simulator = FDTDSimulator(\n",
    "    radius_matrix=radius_matrix\n",
    ")\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.049515:   1%|          | 300/30000 [47:53<82:22:25,  9.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Average Loss: 0.040788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.036160:   1%|▏         | 438/30000 [1:09:20<78:23:42,  9.55s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 14\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m \u001b[43msimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m     17\u001b[0m     _, pred_seq \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_sequence_output(inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/OpTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/OpTorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/workspace/pyDistilledFDTD/simulation/simulator.py:51\u001b[0m, in \u001b[0;36mFDTDSimulator.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     49\u001b[0m batch_output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 51\u001b[0m     batch_output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/workspace/pyDistilledFDTD/simulation/simulator.py:58\u001b[0m, in \u001b[0;36mFDTDSimulator.sim\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_source(x)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_permittivity()\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulation_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m intensities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ports):\n",
      "File \u001b[0;32m~/workspace/pyDistilledFDTD/simulation/fdtd/grid.py:265\u001b[0m, in \u001b[0;36mGrid.run\u001b[0;34m(self, total_time, progress_bar)\u001b[0m\n\u001b[1;32m    263\u001b[0m     time \u001b[38;5;241m=\u001b[39m tqdm(time)\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m time:\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/pyDistilledFDTD/simulation/fdtd/grid.py:271\u001b[0m, in \u001b[0;36mGrid.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"do a single FDTD step by first updating the electric field and then\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    updating the magnetic field\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_E\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_H()\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_steps_passed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/workspace/pyDistilledFDTD/simulation/fdtd/grid.py:295\u001b[0m, in \u001b[0;36mGrid.update_E\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# add sources to grid:\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m src \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msources:\n\u001b[0;32m--> 295\u001b[0m     \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_E\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;66;03m# detect electric field\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetectors:\n",
      "File \u001b[0;32m~/workspace/pyDistilledFDTD/simulation/fdtd/sources.py:297\u001b[0m, in \u001b[0;36mLineSource.update_E\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# do not use list indexing here, as this is much slower especially for torch backend\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;66;03m# DISABLED: self.grid.E[self.x, self.y, self.z, 2] = vect\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, z, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz, vect):\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid\u001b[38;5;241m.\u001b[39mE[x, y, z, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m value\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "total_steps = num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(total=total_steps, desc=\"Training Progress\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for inputs, _ in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_seq = simulator(inputs).detach()\n",
    "\n",
    "        with torch.enable_grad():\n",
    "            _, pred_seq = model.get_sequence_output(inputs)\n",
    "\n",
    "            loss = criterion(pred_seq, target_seq)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        # 更新进度条\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Loss: {loss.item():.6f}\")\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    tqdm.write(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"data/model/lstm_model.pth\"\n",
    "os.makedirs(os.path.dirname(save_dir), exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"radius_matrix\": radius_matrix\n",
    "}, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
